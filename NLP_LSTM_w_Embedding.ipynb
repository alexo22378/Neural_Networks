{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#gpu memory growth fix\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"E:/Python/data/twitter/train.csv\")\n",
    "test_df = pd.read_csv(\"E:/Python/data/twitter/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \n",
       "0                    Just happened a terrible car crash  \n",
       "1     Heard about #earthquake is different cities, s...  \n",
       "2     there is a forest fire at spot pond, geese are...  \n",
       "3              Apocalypse lighting. #Spokane #wildfires  \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                 ...  \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "3259  Storm in RI worse than last hurricane. My city...  \n",
       "3260  Green Line derailment in Chicago http://t.co/U...  \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...  \n",
       "3262  #CityofCalgary has activated its Municipal Eme...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "sentences = list(train_df['text'])\n",
    "for sen in sentences:\n",
    "    X_train.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "sentences = list(test_df['text'])\n",
    "for sen in sentences:\n",
    "    X_test.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMaxLength(lst): \n",
    "    maxLength = max(len(x) for x in lst ) \n",
    "    return maxLength "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen=FindMaxLength(sentences)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "x_train = preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 151)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1948,  122,   89],\n",
       "       [   0,    0,    0, ...,  602, 2061,  213],\n",
       "       [   0,    0,    0, ...,  344,   95,   39],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  855,    2,    1],\n",
       "       [   0,    0,    0, ..., 1917,    2,    1],\n",
       "       [   0,    0,    0, ...,   74,  261, 4566]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/191 [..............................] - ETA: 4:32 - loss: 0.6933 - get_f1: 0.2853WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.419535). Check your callbacks.\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.5827 - get_f1: 0.5196\n",
      "Epoch 00001: val_get_f1 improved from -inf to 0.61962, saving model to best_model.h5\n",
      "191/191 [==============================] - 11s 59ms/step - loss: 0.5822 - get_f1: 0.5208 - val_loss: 0.4792 - val_get_f1: 0.6196\n",
      "Epoch 2/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.4187 - get_f1: 0.7486\n",
      "Epoch 00002: val_get_f1 improved from 0.61962 to 0.63786, saving model to best_model.h5\n",
      "191/191 [==============================] - 8s 43ms/step - loss: 0.4187 - get_f1: 0.7492 - val_loss: 0.4463 - val_get_f1: 0.6379\n",
      "Epoch 3/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.3725 - get_f1: 0.7974\n",
      "Epoch 00003: val_get_f1 improved from 0.63786 to 0.65320, saving model to best_model.h5\n",
      "191/191 [==============================] - 8s 41ms/step - loss: 0.3725 - get_f1: 0.7978 - val_loss: 0.4442 - val_get_f1: 0.6532\n",
      "Epoch 4/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3488 - get_f1: 0.8022\n",
      "Epoch 00004: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 7s 38ms/step - loss: 0.3488 - get_f1: 0.8022 - val_loss: 0.4396 - val_get_f1: 0.6306\n",
      "Epoch 5/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.3288 - get_f1: 0.8245\n",
      "Epoch 00005: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 7s 38ms/step - loss: 0.3290 - get_f1: 0.8244 - val_loss: 0.4530 - val_get_f1: 0.6301\n",
      "Epoch 6/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.3174 - get_f1: 0.8314\n",
      "Epoch 00006: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 7s 38ms/step - loss: 0.3178 - get_f1: 0.8305 - val_loss: 0.4513 - val_get_f1: 0.6435\n",
      "Epoch 7/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.3064 - get_f1: 0.8407\n",
      "Epoch 00007: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 7s 38ms/step - loss: 0.3064 - get_f1: 0.8407 - val_loss: 0.4783 - val_get_f1: 0.6442\n",
      "Epoch 8/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2957 - get_f1: 0.8549\n",
      "Epoch 00008: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 7s 38ms/step - loss: 0.2957 - get_f1: 0.8549 - val_loss: 0.4556 - val_get_f1: 0.6315\n",
      "Epoch 9/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.2840 - get_f1: 0.8538\n",
      "Epoch 00009: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 8s 41ms/step - loss: 0.2837 - get_f1: 0.8546 - val_loss: 0.4705 - val_get_f1: 0.6441\n",
      "Epoch 10/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2708 - get_f1: 0.8617\n",
      "Epoch 00010: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 8s 40ms/step - loss: 0.2708 - get_f1: 0.8617 - val_loss: 0.4972 - val_get_f1: 0.6380\n",
      "Epoch 11/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2638 - get_f1: 0.8718\n",
      "Epoch 00011: val_get_f1 did not improve from 0.65320\n",
      "191/191 [==============================] - 7s 39ms/step - loss: 0.2638 - get_f1: 0.8718 - val_loss: 0.4750 - val_get_f1: 0.6532\n",
      "Epoch 12/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.2512 - get_f1: 0.8768\n",
      "Epoch 00012: val_get_f1 improved from 0.65320 to 0.65901, saving model to best_model.h5\n",
      "191/191 [==============================] - 8s 40ms/step - loss: 0.2509 - get_f1: 0.8774 - val_loss: 0.5608 - val_get_f1: 0.6590\n",
      "Epoch 13/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2417 - get_f1: 0.8834\n",
      "Epoch 00013: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 40ms/step - loss: 0.2417 - get_f1: 0.8834 - val_loss: 0.4601 - val_get_f1: 0.6329\n",
      "Epoch 14/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.2271 - get_f1: 0.8901\n",
      "Epoch 00014: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 39ms/step - loss: 0.2271 - get_f1: 0.8901 - val_loss: 0.5690 - val_get_f1: 0.6500\n",
      "Epoch 15/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.2217 - get_f1: 0.8937\n",
      "Epoch 00015: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.2215 - get_f1: 0.8943 - val_loss: 0.5043 - val_get_f1: 0.6384\n",
      "Epoch 16/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.2085 - get_f1: 0.9015\n",
      "Epoch 00016: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 7s 39ms/step - loss: 0.2083 - get_f1: 0.9020 - val_loss: 0.5323 - val_get_f1: 0.6497\n",
      "Epoch 17/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.1946 - get_f1: 0.9103\n",
      "Epoch 00017: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 41ms/step - loss: 0.1949 - get_f1: 0.9103 - val_loss: 0.5598 - val_get_f1: 0.6441\n",
      "Epoch 18/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.1888 - get_f1: 0.9134\n",
      "Epoch 00018: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 42ms/step - loss: 0.1886 - get_f1: 0.9138 - val_loss: 0.5912 - val_get_f1: 0.6401\n",
      "Epoch 19/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.1856 - get_f1: 0.9147\n",
      "Epoch 00019: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 40ms/step - loss: 0.1856 - get_f1: 0.9147 - val_loss: 0.5634 - val_get_f1: 0.6400\n",
      "Epoch 20/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.1707 - get_f1: 0.9272\n",
      "Epoch 00020: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 42ms/step - loss: 0.1710 - get_f1: 0.9262 - val_loss: 0.5512 - val_get_f1: 0.6214\n",
      "Epoch 21/100\n",
      "190/191 [============================>.] - ETA: 0s - loss: 0.1630 - get_f1: 0.9296\n",
      "Epoch 00021: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 40ms/step - loss: 0.1634 - get_f1: 0.9283 - val_loss: 0.6068 - val_get_f1: 0.6407\n",
      "Epoch 22/100\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.1581 - get_f1: 0.9290\n",
      "Epoch 00022: val_get_f1 did not improve from 0.65901\n",
      "191/191 [==============================] - 8s 40ms/step - loss: 0.1581 - get_f1: 0.9290 - val_loss: 0.6533 - val_get_f1: 0.6391\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x126ac4b57f0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(5000, 32))\n",
    "model.add(layers.LSTM(32, dropout=0.5,\n",
    "    #recurrent_dropout=0.2\n",
    "                     ))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[get_f1])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "es = EarlyStopping(monitor='val_get_f1', mode='max', min_delta=0.001, verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_get_f1', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(x_train, train_df['target'], epochs=100, batch_size=32, callbacks=[\n",
    "    tensorboard, \n",
    "    es, mc\n",
    "    ], \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2295759 ],\n",
       "       [0.7769267 ],\n",
       "       [0.9962835 ],\n",
       "       ...,\n",
       "       [0.96105844],\n",
       "       [0.98051625],\n",
       "       [0.9901377 ]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.round(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zip(test_df.iloc[:, 0], [int(x[0]) for x in yhat])\n",
    "data3 = pd.DataFrame(z, columns=['id', 'target'])\n",
    "data3.to_csv('ss3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
