{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from keras.callbacks import TensorBoard\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from time import time\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "#gpu memory growth fix\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "train_df = pd.read_csv(\"E:/Python/data/twitter/train.csv\")\n",
    "test_df = pd.read_csv(\"E:/Python/data/twitter/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "#     # Correcting spelling\n",
    "#     sentence = correct_spellings(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "sentences = list(train_df['text'])\n",
    "for sen in sentences:\n",
    "    X_train.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "sentences = list(test_df['text'])\n",
    "for sen in sentences:\n",
    "    X_test.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "num_of_words_to_leave = 10000\n",
    "tokenizer = Tokenizer(num_words=num_of_words_to_leave)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the maximum length of a processed tokenized tweet\n",
    "def FindMaxLength(lst): \n",
    "    maxLength = max(len(x) for x in lst ) \n",
    "    return maxLength \n",
    "maxlen=FindMaxLength(sentences)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences\n",
    "from keras import preprocessing\n",
    "x_train = preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom f1-metric for keras\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/179 [..............................] - ETA: 3:31 - loss: 0.6906 - get_f1: 0.4770WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.186571). Check your callbacks.\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.6730 - get_f1: 0.0598\n",
      "Epoch 00001: val_get_f1 improved from -inf to 0.10431, saving model to best_model.h5\n",
      "179/179 [==============================] - 7s 42ms/step - loss: 0.6730 - get_f1: 0.0598 - val_loss: 0.6414 - val_get_f1: 0.1043\n",
      "Epoch 2/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.6203 - get_f1: 0.4957\n",
      "Epoch 00002: val_get_f1 improved from 0.10431 to 0.62333, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.6203 - get_f1: 0.4957 - val_loss: 0.5908 - val_get_f1: 0.6233\n",
      "Epoch 3/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.5674 - get_f1: 0.6350\n",
      "Epoch 00003: val_get_f1 improved from 0.62333 to 0.64328, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.5674 - get_f1: 0.6355 - val_loss: 0.5363 - val_get_f1: 0.6433\n",
      "Epoch 4/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.5235 - get_f1: 0.6980\n",
      "Epoch 00004: val_get_f1 improved from 0.64328 to 0.66357, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.5235 - get_f1: 0.6970 - val_loss: 0.5068 - val_get_f1: 0.6636\n",
      "Epoch 5/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4814 - get_f1: 0.7479\n",
      "Epoch 00005: val_get_f1 improved from 0.66357 to 0.67844, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4812 - get_f1: 0.7484 - val_loss: 0.4956 - val_get_f1: 0.6784\n",
      "Epoch 6/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.4501 - get_f1: 0.7696\n",
      "Epoch 00006: val_get_f1 did not improve from 0.67844\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.4500 - get_f1: 0.7690 - val_loss: 0.4536 - val_get_f1: 0.6663\n",
      "Epoch 7/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4289 - get_f1: 0.7793\n",
      "Epoch 00007: val_get_f1 improved from 0.67844 to 0.69263, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4287 - get_f1: 0.7801 - val_loss: 0.4573 - val_get_f1: 0.6926\n",
      "Epoch 8/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4053 - get_f1: 0.8024\n",
      "Epoch 00008: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4053 - get_f1: 0.8024 - val_loss: 0.4469 - val_get_f1: 0.6888\n",
      "Epoch 9/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3923 - get_f1: 0.8054 E\n",
      "Epoch 00009: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3925 - get_f1: 0.8046 - val_loss: 0.4582 - val_get_f1: 0.6917\n",
      "Epoch 10/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3839 - get_f1: 0.8130\n",
      "Epoch 00010: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3827 - get_f1: 0.8144 - val_loss: 0.4551 - val_get_f1: 0.6898\n",
      "Epoch 11/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3621 - get_f1: 0.8032\n",
      "Epoch 00011: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3621 - get_f1: 0.8032 - val_loss: 0.4428 - val_get_f1: 0.6872\n",
      "Epoch 12/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3487 - get_f1: 0.8184\n",
      "Epoch 00012: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3486 - get_f1: 0.8178 - val_loss: 0.4526 - val_get_f1: 0.6887\n",
      "Epoch 13/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3442 - get_f1: 0.8218\n",
      "Epoch 00013: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3440 - get_f1: 0.8227 - val_loss: 0.4569 - val_get_f1: 0.6868\n",
      "Epoch 14/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3399 - get_f1: 0.8305\n",
      "Epoch 00014: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3394 - get_f1: 0.8300 - val_loss: 0.4833 - val_get_f1: 0.6843\n",
      "Epoch 15/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3221 - get_f1: 0.8405\n",
      "Epoch 00015: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 5s 27ms/step - loss: 0.3224 - get_f1: 0.8400 - val_loss: 0.4775 - val_get_f1: 0.6874\n",
      "Epoch 16/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3216 - get_f1: 0.8386 ETA: 0s - loss: 0.3212 - get_f1: \n",
      "Epoch 00016: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3214 - get_f1: 0.8383 - val_loss: 0.4758 - val_get_f1: 0.6799\n",
      "Epoch 17/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3155 - get_f1: 0.8468\n",
      "Epoch 00017: val_get_f1 did not improve from 0.69263\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3155 - get_f1: 0.8468 - val_loss: 0.4699 - val_get_f1: 0.6798\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/100\n",
      "  2/179 [..............................] - ETA: 4:22 - loss: 0.6917 - get_f1: 0.4229WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.473893). Check your callbacks.\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.6784 - get_f1: 0.0457\n",
      "Epoch 00001: val_get_f1 improved from -inf to 0.10198, saving model to best_model.h5\n",
      "179/179 [==============================] - 8s 45ms/step - loss: 0.6784 - get_f1: 0.0457 - val_loss: 0.6594 - val_get_f1: 0.1020\n",
      "Epoch 2/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.6373 - get_f1: 0.3359\n",
      "Epoch 00002: val_get_f1 improved from 0.10198 to 0.61215, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.6371 - get_f1: 0.3380 - val_loss: 0.6057 - val_get_f1: 0.6121\n",
      "Epoch 3/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5841 - get_f1: 0.5172\n",
      "Epoch 00003: val_get_f1 improved from 0.61215 to 0.64604, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.5841 - get_f1: 0.5172 - val_loss: 0.5531 - val_get_f1: 0.6460\n",
      "Epoch 4/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.5307 - get_f1: 0.6851\n",
      "Epoch 00004: val_get_f1 improved from 0.64604 to 0.65306, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.5298 - get_f1: 0.6872 - val_loss: 0.5135 - val_get_f1: 0.6531\n",
      "Epoch 5/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4831 - get_f1: 0.7353\n",
      "Epoch 00005: val_get_f1 improved from 0.65306 to 0.70969, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4831 - get_f1: 0.7353 - val_loss: 0.5101 - val_get_f1: 0.7097\n",
      "Epoch 6/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4453 - get_f1: 0.7709 ETA: 0s - loss: 0.4465 - get_f1: 0.\n",
      "Epoch 00006: val_get_f1 improved from 0.70969 to 0.71558, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4453 - get_f1: 0.7709 - val_loss: 0.4646 - val_get_f1: 0.7156\n",
      "Epoch 7/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4168 - get_f1: 0.7877\n",
      "Epoch 00007: val_get_f1 did not improve from 0.71558\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4168 - get_f1: 0.7877 - val_loss: 0.4542 - val_get_f1: 0.7072\n",
      "Epoch 8/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3951 - get_f1: 0.8046\n",
      "Epoch 00008: val_get_f1 did not improve from 0.71558\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3951 - get_f1: 0.8046 - val_loss: 0.4559 - val_get_f1: 0.7114\n",
      "Epoch 9/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3708 - get_f1: 0.8199\n",
      "Epoch 00009: val_get_f1 did not improve from 0.71558\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3708 - get_f1: 0.8199 - val_loss: 0.4560 - val_get_f1: 0.7111\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/179 [============================>.] - ETA: 0s - loss: 0.3568 - get_f1: 0.8193\n",
      "Epoch 00010: val_get_f1 improved from 0.71558 to 0.71860, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 22ms/step - loss: 0.3574 - get_f1: 0.8194 - val_loss: 0.4587 - val_get_f1: 0.7186\n",
      "Epoch 11/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3394 - get_f1: 0.8315\n",
      "Epoch 00011: val_get_f1 did not improve from 0.71860\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3388 - get_f1: 0.8324 - val_loss: 0.4650 - val_get_f1: 0.7159\n",
      "Epoch 12/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3231 - get_f1: 0.8423\n",
      "Epoch 00012: val_get_f1 did not improve from 0.71860\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.3230 - get_f1: 0.8425 - val_loss: 0.4691 - val_get_f1: 0.7180\n",
      "Epoch 13/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3260 - get_f1: 0.8423\n",
      "Epoch 00013: val_get_f1 did not improve from 0.71860\n",
      "179/179 [==============================] - 5s 26ms/step - loss: 0.3255 - get_f1: 0.8424 - val_loss: 0.4716 - val_get_f1: 0.7125\n",
      "Epoch 14/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3151 - get_f1: 0.8455\n",
      "Epoch 00014: val_get_f1 did not improve from 0.71860\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3153 - get_f1: 0.8463 - val_loss: 0.4787 - val_get_f1: 0.7156\n",
      "Epoch 15/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3108 - get_f1: 0.8485\n",
      "Epoch 00015: val_get_f1 did not improve from 0.71860\n",
      "179/179 [==============================] - 4s 22ms/step - loss: 0.3104 - get_f1: 0.8488 - val_loss: 0.4853 - val_get_f1: 0.7142\n",
      "Epoch 16/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2998 - get_f1: 0.8560\n",
      "Epoch 00016: val_get_f1 did not improve from 0.71860\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3002 - get_f1: 0.8557 - val_loss: 0.4818 - val_get_f1: 0.7147\n",
      "Epoch 17/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.2950 - get_f1: 0.8594\n",
      "Epoch 00017: val_get_f1 did not improve from 0.71860\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.2956 - get_f1: 0.8592 - val_loss: 0.4991 - val_get_f1: 0.7150\n",
      "Epoch 18/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.2882 - get_f1: 0.8623\n",
      "Epoch 00018: val_get_f1 improved from 0.71860 to 0.72034, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.2882 - get_f1: 0.8629 - val_loss: 0.5079 - val_get_f1: 0.7203\n",
      "Epoch 19/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.2880 - get_f1: 0.8631 ETA: 0s - loss: 0.2819 - ge\n",
      "Epoch 00019: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2880 - get_f1: 0.8631 - val_loss: 0.4974 - val_get_f1: 0.7137\n",
      "Epoch 20/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.2871 - get_f1: 0.8698 ETA: 0s - loss: 0.2858 - get_f1: 0.\n",
      "Epoch 00020: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.2870 - get_f1: 0.8700 - val_loss: 0.5071 - val_get_f1: 0.7148\n",
      "Epoch 21/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2777 - get_f1: 0.8734\n",
      "Epoch 00021: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2779 - get_f1: 0.8727 - val_loss: 0.5101 - val_get_f1: 0.7157\n",
      "Epoch 22/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.2713 - get_f1: 0.8712\n",
      "Epoch 00022: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 6s 35ms/step - loss: 0.2713 - get_f1: 0.8712 - val_loss: 0.5266 - val_get_f1: 0.7194\n",
      "Epoch 23/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.2637 - get_f1: 0.8774\n",
      "Epoch 00023: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 8s 42ms/step - loss: 0.2637 - get_f1: 0.8774 - val_loss: 0.5255 - val_get_f1: 0.7171\n",
      "Epoch 24/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.2703 - get_f1: 0.8780 ETA: 0s - loss: 0.2702 - get_f1: 0.87\n",
      "Epoch 00024: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 6s 33ms/step - loss: 0.2702 - get_f1: 0.8782 - val_loss: 0.5283 - val_get_f1: 0.7115\n",
      "Epoch 25/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2618 - get_f1: 0.8760\n",
      "Epoch 00025: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2615 - get_f1: 0.8767 - val_loss: 0.5330 - val_get_f1: 0.7118\n",
      "Epoch 26/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2601 - get_f1: 0.8755\n",
      "Epoch 00026: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2598 - get_f1: 0.8758 - val_loss: 0.5398 - val_get_f1: 0.7062\n",
      "Epoch 27/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2568 - get_f1: 0.8866 ETA: \n",
      "Epoch 00027: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 5s 26ms/step - loss: 0.2570 - get_f1: 0.8861 - val_loss: 0.5433 - val_get_f1: 0.7117\n",
      "Epoch 28/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.2517 - get_f1: 0.8802\n",
      "Epoch 00028: val_get_f1 did not improve from 0.72034\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2517 - get_f1: 0.8802 - val_loss: 0.5623 - val_get_f1: 0.7130\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/100\n",
      "  2/179 [..............................] - ETA: 4:01 - loss: 0.6928 - get_f1: 0.3125WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.360447). Check your callbacks.\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.6832 - get_f1: 0.0035\n",
      "Epoch 00001: val_get_f1 improved from -inf to 0.00000, saving model to best_model.h5\n",
      "179/179 [==============================] - 8s 44ms/step - loss: 0.6832 - get_f1: 0.0035 - val_loss: 0.6749 - val_get_f1: 0.0000e+00\n",
      "Epoch 2/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.6465 - get_f1: 0.1980\n",
      "Epoch 00002: val_get_f1 improved from 0.00000 to 0.39722, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.6463 - get_f1: 0.2019 - val_loss: 0.6217 - val_get_f1: 0.3972\n",
      "Epoch 3/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5872 - get_f1: 0.6016\n",
      "Epoch 00003: val_get_f1 improved from 0.39722 to 0.61109, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.5872 - get_f1: 0.6016 - val_loss: 0.5709 - val_get_f1: 0.6111\n",
      "Epoch 4/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.5333 - get_f1: 0.6969\n",
      "Epoch 00004: val_get_f1 improved from 0.61109 to 0.67531, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.5335 - get_f1: 0.6970 - val_loss: 0.5328 - val_get_f1: 0.6753\n",
      "Epoch 5/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4881 - get_f1: 0.7449 ETA: 0s - loss: 0.4875 - \n",
      "Epoch 00005: val_get_f1 improved from 0.67531 to 0.69548, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4889 - get_f1: 0.7458 - val_loss: 0.5009 - val_get_f1: 0.6955\n",
      "Epoch 6/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4555 - get_f1: 0.7754\n",
      "Epoch 00006: val_get_f1 improved from 0.69548 to 0.70172, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.4555 - get_f1: 0.7754 - val_loss: 0.4751 - val_get_f1: 0.7017\n",
      "Epoch 7/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4239 - get_f1: 0.7915\n",
      "Epoch 00007: val_get_f1 did not improve from 0.70172\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4239 - get_f1: 0.7915 - val_loss: 0.4637 - val_get_f1: 0.6989\n",
      "Epoch 8/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3945 - get_f1: 0.8043\n",
      "Epoch 00008: val_get_f1 improved from 0.70172 to 0.70412, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3941 - get_f1: 0.8051 - val_loss: 0.4591 - val_get_f1: 0.7041\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/179 [============================>.] - ETA: 0s - loss: 0.3771 - get_f1: 0.8121\n",
      "Epoch 00009: val_get_f1 did not improve from 0.70412\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3772 - get_f1: 0.8115 - val_loss: 0.4603 - val_get_f1: 0.7028\n",
      "Epoch 10/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3711 - get_f1: 0.8106\n",
      "Epoch 00010: val_get_f1 improved from 0.70412 to 0.70925, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3706 - get_f1: 0.8117 - val_loss: 0.4660 - val_get_f1: 0.7092\n",
      "Epoch 11/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3505 - get_f1: 0.8280\n",
      "Epoch 00011: val_get_f1 did not improve from 0.70925\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3503 - get_f1: 0.8276 - val_loss: 0.4661 - val_get_f1: 0.7029\n",
      "Epoch 12/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3395 - get_f1: 0.8336\n",
      "Epoch 00012: val_get_f1 did not improve from 0.70925\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3425 - get_f1: 0.8317 - val_loss: 0.4761 - val_get_f1: 0.7083\n",
      "Epoch 13/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3319 - get_f1: 0.8432\n",
      "Epoch 00013: val_get_f1 did not improve from 0.70925\n",
      "179/179 [==============================] - 5s 25ms/step - loss: 0.3319 - get_f1: 0.8441 - val_loss: 0.4914 - val_get_f1: 0.7065\n",
      "Epoch 14/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3189 - get_f1: 0.8495\n",
      "Epoch 00014: val_get_f1 did not improve from 0.70925\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.3206 - get_f1: 0.8480 - val_loss: 0.4882 - val_get_f1: 0.7056\n",
      "Epoch 15/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3174 - get_f1: 0.8472 ETA: 0s - l\n",
      "Epoch 00015: val_get_f1 improved from 0.70925 to 0.71203, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3174 - get_f1: 0.8472 - val_loss: 0.4993 - val_get_f1: 0.7120\n",
      "Epoch 16/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3133 - get_f1: 0.8517 ETA: 0s - loss: 0.3147 - ge\n",
      "Epoch 00016: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.3133 - get_f1: 0.8517 - val_loss: 0.5017 - val_get_f1: 0.7012\n",
      "Epoch 17/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3030 - get_f1: 0.8511\n",
      "Epoch 00017: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.3030 - get_f1: 0.8511 - val_loss: 0.5162 - val_get_f1: 0.7012\n",
      "Epoch 18/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3011 - get_f1: 0.8525\n",
      "Epoch 00018: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 5s 25ms/step - loss: 0.3009 - get_f1: 0.8525 - val_loss: 0.5151 - val_get_f1: 0.7042\n",
      "Epoch 19/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.2955 - get_f1: 0.8531\n",
      "Epoch 00019: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2955 - get_f1: 0.8531 - val_loss: 0.5158 - val_get_f1: 0.6993\n",
      "Epoch 20/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.2801 - get_f1: 0.8649\n",
      "Epoch 00020: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.2801 - get_f1: 0.8649 - val_loss: 0.5357 - val_get_f1: 0.6995\n",
      "Epoch 21/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.2751 - get_f1: 0.8625\n",
      "Epoch 00021: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2746 - get_f1: 0.8636 - val_loss: 0.5443 - val_get_f1: 0.7017\n",
      "Epoch 22/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.2829 - get_f1: 0.8662\n",
      "Epoch 00022: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.2828 - get_f1: 0.8662 - val_loss: 0.5412 - val_get_f1: 0.6973\n",
      "Epoch 23/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.2729 - get_f1: 0.8678\n",
      "Epoch 00023: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 5s 25ms/step - loss: 0.2729 - get_f1: 0.8680 - val_loss: 0.5592 - val_get_f1: 0.6976\n",
      "Epoch 24/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.2652 - get_f1: 0.8668\n",
      "Epoch 00024: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2652 - get_f1: 0.8668 - val_loss: 0.5762 - val_get_f1: 0.6960\n",
      "Epoch 25/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2655 - get_f1: 0.8680\n",
      "Epoch 00025: val_get_f1 did not improve from 0.71203\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.2652 - get_f1: 0.8688 - val_loss: 0.5579 - val_get_f1: 0.7002\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/100\n",
      "  2/179 [..............................] - ETA: 4:09 - loss: 0.6925 - get_f1: 0.1769WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.401591). Check your callbacks.\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.6782 - get_f1: 0.0167\n",
      "Epoch 00001: val_get_f1 improved from -inf to 0.00333, saving model to best_model.h5\n",
      "179/179 [==============================] - 8s 46ms/step - loss: 0.6781 - get_f1: 0.0172 - val_loss: 0.6607 - val_get_f1: 0.0033\n",
      "Epoch 2/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.6357 - get_f1: 0.3545\n",
      "Epoch 00002: val_get_f1 improved from 0.00333 to 0.51175, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.6357 - get_f1: 0.3545 - val_loss: 0.6063 - val_get_f1: 0.5117\n",
      "Epoch 3/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5819 - get_f1: 0.6142\n",
      "Epoch 00003: val_get_f1 improved from 0.51175 to 0.58179, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.5819 - get_f1: 0.6142 - val_loss: 0.5537 - val_get_f1: 0.5818\n",
      "Epoch 4/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.5234 - get_f1: 0.6838\n",
      "Epoch 00004: val_get_f1 improved from 0.58179 to 0.66759, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.5234 - get_f1: 0.6838 - val_loss: 0.5157 - val_get_f1: 0.6676\n",
      "Epoch 5/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.4900 - get_f1: 0.7389\n",
      "Epoch 00005: val_get_f1 improved from 0.66759 to 0.68590, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4900 - get_f1: 0.7389 - val_loss: 0.4860 - val_get_f1: 0.6859\n",
      "Epoch 6/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4497 - get_f1: 0.7722\n",
      "Epoch 00006: val_get_f1 improved from 0.68590 to 0.71278, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.4498 - get_f1: 0.7730 - val_loss: 0.4713 - val_get_f1: 0.7128\n",
      "Epoch 7/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.4247 - get_f1: 0.7857\n",
      "Epoch 00007: val_get_f1 did not improve from 0.71278\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.4245 - get_f1: 0.7852 - val_loss: 0.4539 - val_get_f1: 0.7088\n",
      "Epoch 8/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3969 - get_f1: 0.8002\n",
      "Epoch 00008: val_get_f1 improved from 0.71278 to 0.72171, saving model to best_model.h5\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3969 - get_f1: 0.7994 - val_loss: 0.4491 - val_get_f1: 0.7217\n",
      "Epoch 9/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3791 - get_f1: 0.8145\n",
      "Epoch 00009: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 5s 27ms/step - loss: 0.3786 - get_f1: 0.8156 - val_loss: 0.4457 - val_get_f1: 0.7054\n",
      "Epoch 10/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3609 - get_f1: 0.8229 ETA: 0s - loss: 0.3635 - get_f1\n",
      "Epoch 00010: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 4s 24ms/step - loss: 0.3616 - get_f1: 0.8213 - val_loss: 0.4475 - val_get_f1: 0.7053\n",
      "Epoch 11/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3465 - get_f1: 0.8229\n",
      "Epoch 00011: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 4s 25ms/step - loss: 0.3469 - get_f1: 0.8221 - val_loss: 0.4522 - val_get_f1: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "177/179 [============================>.] - ETA: 0s - loss: 0.3401 - get_f1: 0.8324\n",
      "Epoch 00012: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3394 - get_f1: 0.8333 - val_loss: 0.4575 - val_get_f1: 0.7117\n",
      "Epoch 13/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3294 - get_f1: 0.8398\n",
      "Epoch 00013: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 5s 26ms/step - loss: 0.3294 - get_f1: 0.8398 - val_loss: 0.4625 - val_get_f1: 0.7039\n",
      "Epoch 14/100\n",
      "179/179 [==============================] - ETA: 0s - loss: 0.3183 - get_f1: 0.8426\n",
      "Epoch 00014: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 6s 32ms/step - loss: 0.3183 - get_f1: 0.8426 - val_loss: 0.4636 - val_get_f1: 0.7078\n",
      "Epoch 15/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3041 - get_f1: 0.8488\n",
      "Epoch 00015: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 9s 51ms/step - loss: 0.3040 - get_f1: 0.8492 - val_loss: 0.4777 - val_get_f1: 0.7062\n",
      "Epoch 16/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.3045 - get_f1: 0.8524\n",
      "Epoch 00016: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 6s 32ms/step - loss: 0.3045 - get_f1: 0.8519 - val_loss: 0.4856 - val_get_f1: 0.7075\n",
      "Epoch 17/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2891 - get_f1: 0.8606\n",
      "Epoch 00017: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 4s 22ms/step - loss: 0.2885 - get_f1: 0.8614 - val_loss: 0.4934 - val_get_f1: 0.6975\n",
      "Epoch 18/100\n",
      "178/179 [============================>.] - ETA: 0s - loss: 0.2942 - get_f1: 0.8558\n",
      "Epoch 00018: val_get_f1 did not improve from 0.72171\n",
      "179/179 [==============================] - 4s 21ms/step - loss: 0.2942 - get_f1: 0.8553 - val_loss: 0.4936 - val_get_f1: 0.6999\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model and predicting in 4-fold cv-mode\n",
    "train_y_enc = train_df['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import scipy\n",
    "first_col = True\n",
    "cross_fold = KFold(n_splits = 4, shuffle=True)\n",
    "for train_index, test_index in cross_fold.split(x_train):\n",
    "    validation_X, validation_y = x_train[test_index], train_y_enc[test_index]\n",
    "    train_X, train_y = x_train[train_index], train_y_enc[train_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(num_of_words_to_leave, 4))\n",
    "    model.add(layers.LSTM(4, #return_sequences = True\n",
    "        #recurrent_dropout=0.2\n",
    "                         ))\n",
    "    #model.add(layers.GlobalMaxPool1D())\n",
    "    # model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    # model.add(layers.Dense(128, activation = \"relu\"))\n",
    "    # model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[get_f1])\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "    es = EarlyStopping(monitor='val_get_f1', mode='max', min_delta=0.001, verbose=1, patience=10)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_get_f1', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    # fit model and save the best\n",
    "    history = model.fit(train_X, train_y, epochs=100, batch_size=32, callbacks=[\n",
    "        tensorboard, \n",
    "        es, mc\n",
    "        ], \n",
    "        validation_data=(validation_X, validation_y),\n",
    "    )\n",
    "    saved_model = load_model('best_model.h5', custom_objects={\"get_f1\": get_f1})\n",
    "    \n",
    "    probs = saved_model.predict(x_test)\n",
    "    predict = [x[0] for x in probs]\n",
    "    if first_col:\n",
    "        pr_values = np.array(predict, ndmin=2)\n",
    "        pr_values = np.transpose(pr_values)\n",
    "        first_col = False\n",
    "    else:\n",
    "        pr_values = np.insert(pr_values, -1, predict, axis=1)\n",
    "pr_values= np.mean(pr_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65323895, 0.61684036, 0.8721913 , ..., 0.8428601 , 0.91084456,\n",
       "       0.7592432 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities\n",
    "pr_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the result\n",
    "z = zip(test_df.iloc[:, 0], [int(x) for x in np.round(pr_values)])\n",
    "data3 = pd.DataFrame(z, columns=['id', 'target'])\n",
    "data3.to_csv('ss10.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
